{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GRtIriYeLwJa"
   },
   "source": [
    "# Stemming\n",
    "\n",
    "For NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 68,
     "status": "ok",
     "timestamp": 1748660297601,
     "user": {
      "displayName": "Kishan Prakash",
      "userId": "00782312557775250381"
     },
     "user_tz": -330
    },
    "id": "J9JuIofCP2Vm",
    "outputId": "44a382c2-2d56-415d-d3e1-e1db50deaadf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original : running --------> Stemmed : run\n",
      "Original : happily --------> Stemmed : happili\n",
      "Original : studies --------> Stemmed : studi\n",
      "Original : connection --------> Stemmed : connect\n",
      "Original : argument --------> Stemmed : argument\n",
      "Original : national --------> Stemmed : nation\n",
      "Original : bouncing --------> Stemmed : bounc\n",
      "Original : relational --------> Stemmed : relat\n",
      "Original : cats --------> Stemmed : cat\n",
      "Original : leaves --------> Stemmed : leav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kisha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Porter Stemmer\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('punkt')\n",
    "\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "words = [\"running\", \"happily\", \"studies\", \"connection\",\n",
    "         \"argument\", \"national\", \"bouncing\", \"relational\", \"cats\", \"leaves\"]\n",
    "\n",
    "for word in words:\n",
    "\n",
    "    print(f\"Original : {word} --------> Stemmed : {porter_stemmer.stem(word)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1748660318963,
     "user": {
      "displayName": "Kishan Prakash",
      "userId": "00782312557775250381"
     },
     "user_tz": -330
    },
    "id": "YdrTs01zL6Ed",
    "outputId": "96fab9f5-41a8-46ac-e067-076a203c8344"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original : running --------> Stemmed : run\n",
      "Original : happily --------> Stemmed : happili\n",
      "Original : studies --------> Stemmed : studi\n",
      "Original : connection --------> Stemmed : connect\n",
      "Original : argument --------> Stemmed : argument\n",
      "Original : national --------> Stemmed : nation\n",
      "Original : bouncing --------> Stemmed : bounc\n",
      "Original : relational --------> Stemmed : relat\n",
      "Original : cats --------> Stemmed : cat\n",
      "Original : leaves --------> Stemmed : leav\n"
     ]
    }
   ],
   "source": [
    "# Snowball Stemmer\n",
    "\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "snowball_stemmer = SnowballStemmer(language='english')\n",
    "\n",
    "words = [\"running\", \"happily\", \"studies\", \"connection\",\n",
    "         \"argument\", \"national\", \"bouncing\", \"relational\", \"cats\", \"leaves\"]\n",
    "\n",
    "for word in words:\n",
    "\n",
    "    print(f\"Original : {word} --------> Stemmed : {snowball_stemmer.stem(word)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1748660653204,
     "user": {
      "displayName": "Kishan Prakash",
      "userId": "00782312557775250381"
     },
     "user_tz": -330
    },
    "id": "xTn_oF3ZL5_r",
    "outputId": "f0e7f571-cadc-4a40-fe11-bdef4c60dd2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porter :  fairli\n",
      "Snowball :  fair\n",
      "Porter :  sportingli\n",
      "Snowball :  sport\n",
      "Porter :  go\n",
      "Snowball :  go\n"
     ]
    }
   ],
   "source": [
    "print(\"Porter : \", porter_stemmer.stem('fairly'))\n",
    "print(\"Snowball : \",snowball_stemmer.stem('fairly'))\n",
    "\n",
    "print(\"Porter : \", porter_stemmer.stem('sportingly'))\n",
    "print(\"Snowball : \",snowball_stemmer.stem('sportingly'))\n",
    "\n",
    "print(\"Porter : \", porter_stemmer.stem('going'))\n",
    "print(\"Snowball : \",snowball_stemmer.stem('going'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1748660976455,
     "user": {
      "displayName": "Kishan Prakash",
      "userId": "00782312557775250381"
     },
     "user_tz": -330
    },
    "id": "hVk1vsYLLqFX",
    "outputId": "b5ac1430-5c42-4f05-e24b-9baa83e05d80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word : relate --------> Stemmed : relat\n",
      "Word : relativity --------> Stemmed : rel\n"
     ]
    }
   ],
   "source": [
    "word_1 = \"relate\"\n",
    "word_2 = \"relativity\"\n",
    "\n",
    "print(f\"Word : {word_1} --------> Stemmed : {porter_stemmer.stem(word_1)}\")\n",
    "print(f\"Word : {word_2} --------> Stemmed : {porter_stemmer.stem(word_2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1748661007586,
     "user": {
      "displayName": "Kishan Prakash",
      "userId": "00782312557775250381"
     },
     "user_tz": -330
    },
    "id": "MYVWb6-yURUr",
    "outputId": "76263dba-b6aa-4686-c318-5e2604331b23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word : probe --------> Stemmed : probe\n",
      "Word : probate --------> Stemmed : probat\n"
     ]
    }
   ],
   "source": [
    "word_1 = \"probe\"\n",
    "word_2 = \"probate\"\n",
    "\n",
    "print(f\"Word : {word_1} --------> Stemmed : {porter_stemmer.stem(word_1)}\")\n",
    "print(f\"Word : {word_2} --------> Stemmed : {porter_stemmer.stem(word_2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C7DDCdlLMQRF"
   },
   "source": [
    "# Lemmatization\n",
    "For spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 411,
     "status": "ok",
     "timestamp": 1748662056500,
     "user": {
      "displayName": "Kishan Prakash",
      "userId": "00782312557775250381"
     },
     "user_tz": -330
    },
    "id": "psqhK_COMaek",
    "outputId": "293360fc-55c4-4592-d904-cbacec910adf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running run\n",
      "happily happily\n",
      "studies study\n",
      "connection connection\n",
      "argument argument\n",
      "national national\n",
      "bouncing bounce\n",
      "relational relational\n",
      "cats cat\n",
      "leaves leave\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "doc = nlp(u\"running happily studies connection argument national bouncing relational cats leaves\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 37,
     "status": "ok",
     "timestamp": 1748662708224,
     "user": {
      "displayName": "Kishan Prakash",
      "userId": "00782312557775250381"
     },
     "user_tz": -330
    },
    "id": "lqggvCaOYS7G",
    "outputId": "6d61d1e3-1ad3-4c1e-f121-638f165ff2c5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\kisha\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "going\n",
      "going\n",
      "go\n"
     ]
    }
   ],
   "source": [
    "# NLTK\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(lemmatizer.lemmatize('going'))\n",
    "\n",
    "print(lemmatizer.lemmatize('going', pos='n'))\n",
    "\n",
    "print(lemmatizer.lemmatize('going', pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 296,
     "status": "ok",
     "timestamp": 1748662739870,
     "user": {
      "displayName": "Kishan Prakash",
      "userId": "00782312557775250381"
     },
     "user_tz": -330
    },
    "id": "D2_c_rpZZxzV",
    "outputId": "3be38ec5-6364-40c0-c086-f52d9857e479"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word : He => Stemmed from : He\n",
      "Word : is => Stemmed from : is\n",
      "Word : playing => Stemmed from : playing\n",
      "Word : the => Stemmed from : the\n",
      "Word : music => Stemmed from : music\n",
      "Word : beatifully => Stemmed from : beatifully\n",
      "Word : . => Stemmed from : .\n",
      "Word : She => Stemmed from : She\n",
      "Word : painted => Stemmed from : painted\n",
      "Word : this => Stemmed from : this\n",
      "Word : picture => Stemmed from : picture\n",
      "Word : so => Stemmed from : so\n",
      "Word : well => Stemmed from : well\n",
      "Word : . => Stemmed from : .\n",
      "Word : They => Stemmed from : They\n",
      "Word : will => Stemmed from : will\n",
      "Word : be => Stemmed from : be\n",
      "Word : going => Stemmed from : going\n",
      "Word : to => Stemmed from : to\n",
      "Word : Mumbai => Stemmed from : Mumbai\n",
      "Word : tommorrow => Stemmed from : tommorrow\n",
      "Word : . => Stemmed from : .\n",
      "Word : By => Stemmed from : By\n",
      "Word : when => Stemmed from : when\n",
      "Word : will => Stemmed from : will\n",
      "Word : they => Stemmed from : they\n",
      "Word : have => Stemmed from : have\n",
      "Word : returned => Stemmed from : returned\n",
      "Word : . => Stemmed from : .\n",
      "Word : Historically => Stemmed from : Historically\n",
      "Word : batting => Stemmed from : batting\n",
      "Word : is => Stemmed from : is\n",
      "Word : better => Stemmed from : better\n",
      "Word : than => Stemmed from : than\n",
      "Word : bowling => Stemmed from : bowling\n",
      "Word : first => Stemmed from : first\n",
      "Word : . => Stemmed from : .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\kisha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "words = \"\"\"\n",
    "He is playing the music beatifully.\n",
    "She painted this picture so well.\n",
    "They will be going to Mumbai tommorrow.\n",
    "By when will they have returned.\n",
    "Historically batting is better than bowling first.\"\"\"\n",
    "\n",
    "tokenized_words = word_tokenize(words)\n",
    "\n",
    "for word in tokenized_words:\n",
    "\n",
    "    print(f\"Word : {word} => Stemmed from : {lemmatizer.lemmatize(word)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1748662796251,
     "user": {
      "displayName": "Kishan Prakash",
      "userId": "00782312557775250381"
     },
     "user_tz": -330
    },
    "id": "j5X3y0YsazBl",
    "outputId": "479a3cbf-0e20-4966-bee9-1d4c88659697"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word : He => Stemmed from : He\n",
      "Word : is => Stemmed from : be\n",
      "Word : playing => Stemmed from : play\n",
      "Word : the => Stemmed from : the\n",
      "Word : music => Stemmed from : music\n",
      "Word : beatifully => Stemmed from : beatifully\n",
      "Word : . => Stemmed from : .\n",
      "Word : She => Stemmed from : She\n",
      "Word : painted => Stemmed from : paint\n",
      "Word : this => Stemmed from : this\n",
      "Word : picture => Stemmed from : picture\n",
      "Word : so => Stemmed from : so\n",
      "Word : well => Stemmed from : well\n",
      "Word : . => Stemmed from : .\n",
      "Word : They => Stemmed from : They\n",
      "Word : will => Stemmed from : will\n",
      "Word : be => Stemmed from : be\n",
      "Word : going => Stemmed from : go\n",
      "Word : to => Stemmed from : to\n",
      "Word : Mumbai => Stemmed from : Mumbai\n",
      "Word : tommorrow => Stemmed from : tommorrow\n",
      "Word : . => Stemmed from : .\n",
      "Word : By => Stemmed from : By\n",
      "Word : when => Stemmed from : when\n",
      "Word : will => Stemmed from : will\n",
      "Word : they => Stemmed from : they\n",
      "Word : have => Stemmed from : have\n",
      "Word : returned => Stemmed from : return\n",
      "Word : . => Stemmed from : .\n",
      "Word : Historically => Stemmed from : Historically\n",
      "Word : batting => Stemmed from : bat\n",
      "Word : is => Stemmed from : be\n",
      "Word : better => Stemmed from : better\n",
      "Word : than => Stemmed from : than\n",
      "Word : bowling => Stemmed from : bowl\n",
      "Word : first => Stemmed from : first\n",
      "Word : . => Stemmed from : .\n"
     ]
    }
   ],
   "source": [
    "for word in tokenized_words:\n",
    "\n",
    "    print(f\"Word : {word} => Stemmed from : {lemmatizer.lemmatize(word, pos='v')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 467,
     "status": "ok",
     "timestamp": 1748663342122,
     "user": {
      "displayName": "Kishan Prakash",
      "userId": "00782312557775250381"
     },
     "user_tz": -330
    },
    "id": "HfUk5ASLbFkD",
    "outputId": "2d359d4c-4970-4b3d-f35c-52382594670e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original : The \t Lemmatized : the\n",
      "----------------------------------------\n",
      "Original : cats \t Lemmatized : cat\n",
      "----------------------------------------\n",
      "Original : are \t Lemmatized : be\n",
      "----------------------------------------\n",
      "Original : running \t Lemmatized : run\n",
      "----------------------------------------\n",
      "Original : in \t Lemmatized : in\n",
      "----------------------------------------\n",
      "Original : the \t Lemmatized : the\n",
      "----------------------------------------\n",
      "Original : garden \t Lemmatized : garden\n",
      "----------------------------------------\n",
      "Original : . \t Lemmatized : .\n",
      "----------------------------------------\n",
      "Original : She \t Lemmatized : she\n",
      "----------------------------------------\n",
      "Original : bettered \t Lemmatized : better\n",
      "----------------------------------------\n",
      "Original : her \t Lemmatized : her\n",
      "----------------------------------------\n",
      "Original : skills \t Lemmatized : skill\n",
      "----------------------------------------\n",
      "Original : by \t Lemmatized : by\n",
      "----------------------------------------\n",
      "Original : practicing \t Lemmatized : practice\n",
      "----------------------------------------\n",
      "Original : regularly \t Lemmatized : regularly\n",
      "----------------------------------------\n",
      "Original : . \t Lemmatized : .\n",
      "----------------------------------------\n",
      "Original : Mice \t Lemmatized : mouse\n",
      "----------------------------------------\n",
      "Original : and \t Lemmatized : and\n",
      "----------------------------------------\n",
      "Original : rats \t Lemmatized : rat\n",
      "----------------------------------------\n",
      "Original : can \t Lemmatized : can\n",
      "----------------------------------------\n",
      "Original : be \t Lemmatized : be\n",
      "----------------------------------------\n",
      "Original : found \t Lemmatized : find\n",
      "----------------------------------------\n",
      "Original : in \t Lemmatized : in\n",
      "----------------------------------------\n",
      "Original : the \t Lemmatized : the\n",
      "----------------------------------------\n",
      "Original : bins \t Lemmatized : bin\n",
      "----------------------------------------\n",
      "Original : . \t Lemmatized : .\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# spaCy\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "sentences = [\"The cats are running in the garden.\",\n",
    "             \"She bettered her skills by practicing regularly.\",\n",
    "             \"Mice and rats can be found in the bins.\"]\n",
    "\n",
    "for sentence  in sentences:\n",
    "\n",
    "    doc = nlp(sentence)\n",
    "\n",
    "    for token in doc:\n",
    "\n",
    "        print(f\"Original : {token.text} \\t Lemmatized : {token.lemma_}\")\n",
    "        print(\"-\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 72,
     "status": "ok",
     "timestamp": 1748663413976,
     "user": {
      "displayName": "Kishan Prakash",
      "userId": "00782312557775250381"
     },
     "user_tz": -330
    },
    "id": "Kh8dE66hdgPF",
    "outputId": "dc997cd5-76cb-440a-f950-3cfe9c99a8d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I \t PRON \t 4690420944186131903 \t I\n",
      "am \t AUX \t 10382539506755952630 \t be\n",
      "a \t DET \t 11901859001352538922 \t a\n",
      "runner \t NOUN \t 12640964157389618806 \t runner\n",
      "in \t ADP \t 3002984154512732771 \t in\n",
      "a \t DET \t 11901859001352538922 \t a\n",
      "race \t NOUN \t 8048469955494714898 \t race\n",
      "because \t SCONJ \t 16950148841647037698 \t because\n",
      "I \t PRON \t 4690420944186131903 \t I\n",
      "love \t VERB \t 3702023516439754181 \t love\n",
      "to \t PART \t 3791531372978436496 \t to\n",
      "run \t VERB \t 12767647472892411841 \t run\n",
      "since \t SCONJ \t 10066841407251338481 \t since\n",
      "I \t PRON \t 4690420944186131903 \t I\n",
      "ran \t VERB \t 12767647472892411841 \t run\n",
      "today \t NOUN \t 11042482332948150395 \t today\n"
     ]
    }
   ],
   "source": [
    "doc1 = nlp(u\"I am a runner in a race because I love to run since I ran today\")\n",
    "\n",
    "for token in doc1:\n",
    "\n",
    "    print(token.text, '\\t', token.pos_, '\\t', token.lemma, '\\t', token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EuGQ_RFYepcT"
   },
   "source": [
    "# Span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 760,
     "status": "ok",
     "timestamp": 1748663924180,
     "user": {
      "displayName": "Kishan Prakash",
      "userId": "00782312557775250381"
     },
     "user_tz": -330
    },
    "id": "zzQhsj3iepKS",
    "outputId": "9cf0b6e2-9062-43e2-abe2-e7bef9836788"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a great tool for\n",
      "2\n",
      "6\n",
      "<class 'spacy.tokens.span.Span'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n"
     ]
    }
   ],
   "source": [
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "doc = nlp(u\"SpaCy is a great tool for natural language processing.\")\n",
    "\n",
    "span = Span(doc, start=2, end=6)\n",
    "\n",
    "print(span.text)\n",
    "print(span.start)\n",
    "print(span.end)\n",
    "print(type(span))\n",
    "print(type(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1748663984385,
     "user": {
      "displayName": "Kishan Prakash",
      "userId": "00782312557775250381"
     },
     "user_tz": -330
    },
    "id": "87h2EfQxdqG2",
    "outputId": "3ffe0eae-76ed-4fa4-95c4-fedb5078e31b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a DET det\n",
      "great ADJ amod\n",
      "tool NOUN attr\n",
      "for ADP prep\n"
     ]
    }
   ],
   "source": [
    "for token in span:\n",
    "\n",
    "    print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Vli2E9ZqpSn"
   },
   "source": [
    "# Sentences / Tokenizer types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 398
    },
    "executionInfo": {
     "elapsed": 68,
     "status": "ok",
     "timestamp": 1748664831827,
     "user": {
      "displayName": "Kishan Prakash",
      "userId": "00782312557775250381"
     },
     "user_tz": -330
    },
    "id": "LQ8xhzAjdqEL",
    "outputId": "16c41086-78bc-4fb4-d089-9971a92d7c10"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Pets change our lives &amp; become a part of o...\n",
       "1    Another spot of our #morethanmedicine bus in #...\n",
       "2    What a great team ‚Å¶@HealthSourceOH‚Å© ‚Å¶@Local12‚Å©...\n",
       "3    What a great team ‚Å¶@HealthSourceOH‚Å© ‚Å¶@Local12‚Å©...\n",
       "4    What a great team ‚Å¶@HealthSourceOH‚Å© ‚Å¶@Local12‚Å©...\n",
       "5    What a great team ‚Å¶@HealthSourceOH‚Å© ‚Å¶@Local12‚Å©...\n",
       "6    Will you be at #FIX19? Want a preview of @AG_E...\n",
       "7    Will you be at #FIX19? Want a preview of @AG_E...\n",
       "8    Will you be at #FIX19? Want a preview of @AG_E...\n",
       "9    Will you be at #FIX19? Want a preview of @AG_E...\n",
       "Name: Tweet Content, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import TweetTokenizer, word_tokenize\n",
    "\n",
    "tweets_df = pd.read_csv(\"tweets.csv\")\n",
    "\n",
    "tweet_tokenizer = TweetTokenizer()\n",
    "\n",
    "first_10_tweets = tweets_df['Tweet Content'].dropna().astype(str).head(10)\n",
    "\n",
    "first_10_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1748664875195,
     "user": {
      "displayName": "Kishan Prakash",
      "userId": "00782312557775250381"
     },
     "user_tz": -330
    },
    "id": "9MQGeQY9dp_9",
    "outputId": "3955a884-9490-43fd-a5f4-d0e7081845d3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Pets',\n",
       "  'change',\n",
       "  'our',\n",
       "  'lives',\n",
       "  '&',\n",
       "  'become',\n",
       "  'a',\n",
       "  'part',\n",
       "  'of',\n",
       "  'our',\n",
       "  'families',\n",
       "  '‚ù§',\n",
       "  'Ô∏è',\n",
       "  \"That's\",\n",
       "  'why',\n",
       "  'our',\n",
       "  'members',\n",
       "  'offer',\n",
       "  'many',\n",
       "  'solutions',\n",
       "  'to',\n",
       "  'help',\n",
       "  'you',\n",
       "  'to',\n",
       "  'enjoy',\n",
       "  'a',\n",
       "  'long-lasting',\n",
       "  'bond',\n",
       "  'with',\n",
       "  'your',\n",
       "  'happy',\n",
       "  '&',\n",
       "  'healthy',\n",
       "  'pet',\n",
       "  'üê±',\n",
       "  'üê∂',\n",
       "  '#MorethanMedicine',\n",
       "  '#PetCare',\n",
       "  '#PetsareFamily',\n",
       "  'https://t.co/fZNIXge9a3'],\n",
       " ['Another',\n",
       "  'spot',\n",
       "  'of',\n",
       "  'our',\n",
       "  '#morethanmedicine',\n",
       "  'bus',\n",
       "  'in',\n",
       "  '#bristol',\n",
       "  'this',\n",
       "  'week',\n",
       "  '!',\n",
       "  'If',\n",
       "  'you',\n",
       "  'need',\n",
       "  'support',\n",
       "  'with',\n",
       "  'your',\n",
       "  'cancer',\n",
       "  'diagnosis',\n",
       "  'call',\n",
       "  'us',\n",
       "  'on',\n",
       "  '0303 3000',\n",
       "  '118',\n",
       "  '.',\n",
       "  '#livingwellwithcancer',\n",
       "  'https://t.co/eZGLz0BkXB'],\n",
       " ['What',\n",
       "  'a',\n",
       "  'great',\n",
       "  'team',\n",
       "  '\\u2066',\n",
       "  '@HealthSourceOH',\n",
       "  '\\u2069',\n",
       "  '\\u2066',\n",
       "  '@Local12',\n",
       "  '\\u2069',\n",
       "  '#morethanmedicine',\n",
       "  'https://t.co/g2YzMDUpVA'],\n",
       " ['What',\n",
       "  'a',\n",
       "  'great',\n",
       "  'team',\n",
       "  '\\u2066',\n",
       "  '@HealthSourceOH',\n",
       "  '\\u2069',\n",
       "  '\\u2066',\n",
       "  '@Local12',\n",
       "  '\\u2069',\n",
       "  '#morethanmedicine',\n",
       "  'https://t.co/g2YzMDUpVA'],\n",
       " ['What',\n",
       "  'a',\n",
       "  'great',\n",
       "  'team',\n",
       "  '\\u2066',\n",
       "  '@HealthSourceOH',\n",
       "  '\\u2069',\n",
       "  '\\u2066',\n",
       "  '@Local12',\n",
       "  '\\u2069',\n",
       "  '#morethanmedicine',\n",
       "  'https://t.co/g2YzMDUpVA'],\n",
       " ['What',\n",
       "  'a',\n",
       "  'great',\n",
       "  'team',\n",
       "  '\\u2066',\n",
       "  '@HealthSourceOH',\n",
       "  '\\u2069',\n",
       "  '\\u2066',\n",
       "  '@Local12',\n",
       "  '\\u2069',\n",
       "  '#morethanmedicine',\n",
       "  'https://t.co/g2YzMDUpVA'],\n",
       " ['Will',\n",
       "  'you',\n",
       "  'be',\n",
       "  'at',\n",
       "  '#FIX19',\n",
       "  '?',\n",
       "  'Want',\n",
       "  'a',\n",
       "  'preview',\n",
       "  'of',\n",
       "  '@AG_EM33',\n",
       "  'story',\n",
       "  '?',\n",
       "  'Then',\n",
       "  'check',\n",
       "  'back',\n",
       "  'Monday',\n",
       "  'for',\n",
       "  '#ChangeOfHeart',\n",
       "  'where',\n",
       "  'we',\n",
       "  'sat',\n",
       "  'down',\n",
       "  'with',\n",
       "  'Alin',\n",
       "  'to',\n",
       "  'discuss',\n",
       "  'her',\n",
       "  'new',\n",
       "  'life',\n",
       "  'since',\n",
       "  'her',\n",
       "  'heart',\n",
       "  '#transplant',\n",
       "  '#MoreThanMedicine',\n",
       "  'https://t.co/Xl9zjr7kZ1'],\n",
       " ['Will',\n",
       "  'you',\n",
       "  'be',\n",
       "  'at',\n",
       "  '#FIX19',\n",
       "  '?',\n",
       "  'Want',\n",
       "  'a',\n",
       "  'preview',\n",
       "  'of',\n",
       "  '@AG_EM33',\n",
       "  'story',\n",
       "  '?',\n",
       "  'Then',\n",
       "  'check',\n",
       "  'back',\n",
       "  'Monday',\n",
       "  'for',\n",
       "  '#ChangeOfHeart',\n",
       "  'where',\n",
       "  'we',\n",
       "  'sat',\n",
       "  'down',\n",
       "  'with',\n",
       "  'Alin',\n",
       "  'to',\n",
       "  'discuss',\n",
       "  'her',\n",
       "  'new',\n",
       "  'life',\n",
       "  'since',\n",
       "  'her',\n",
       "  'heart',\n",
       "  '#transplant',\n",
       "  '#MoreThanMedicine',\n",
       "  'https://t.co/Xl9zjr7kZ1'],\n",
       " ['Will',\n",
       "  'you',\n",
       "  'be',\n",
       "  'at',\n",
       "  '#FIX19',\n",
       "  '?',\n",
       "  'Want',\n",
       "  'a',\n",
       "  'preview',\n",
       "  'of',\n",
       "  '@AG_EM33',\n",
       "  'story',\n",
       "  '?',\n",
       "  'Then',\n",
       "  'check',\n",
       "  'back',\n",
       "  'Monday',\n",
       "  'for',\n",
       "  '#ChangeOfHeart',\n",
       "  'where',\n",
       "  'we',\n",
       "  'sat',\n",
       "  'down',\n",
       "  'with',\n",
       "  'Alin',\n",
       "  'to',\n",
       "  'discuss',\n",
       "  'her',\n",
       "  'new',\n",
       "  'life',\n",
       "  'since',\n",
       "  'her',\n",
       "  'heart',\n",
       "  '#transplant',\n",
       "  '#MoreThanMedicine',\n",
       "  'https://t.co/Xl9zjr7kZ1'],\n",
       " ['Will',\n",
       "  'you',\n",
       "  'be',\n",
       "  'at',\n",
       "  '#FIX19',\n",
       "  '?',\n",
       "  'Want',\n",
       "  'a',\n",
       "  'preview',\n",
       "  'of',\n",
       "  '@AG_EM33',\n",
       "  'story',\n",
       "  '?',\n",
       "  'Then',\n",
       "  'check',\n",
       "  'back',\n",
       "  'Monday',\n",
       "  'for',\n",
       "  '#ChangeOfHeart',\n",
       "  'where',\n",
       "  'we',\n",
       "  'sat',\n",
       "  'down',\n",
       "  'with',\n",
       "  'Alin',\n",
       "  'to',\n",
       "  'discuss',\n",
       "  'her',\n",
       "  'new',\n",
       "  'life',\n",
       "  'since',\n",
       "  'her',\n",
       "  'heart',\n",
       "  '#transplant',\n",
       "  '#MoreThanMedicine',\n",
       "  'https://t.co/Xl9zjr7kZ1']]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_tokenized_tweet = [tweet_tokenizer.tokenize(tweet) for tweet in first_10_tweets]\n",
    "\n",
    "tweet_tokenized_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 32,
     "status": "ok",
     "timestamp": 1748664903549,
     "user": {
      "displayName": "Kishan Prakash",
      "userId": "00782312557775250381"
     },
     "user_tz": -330
    },
    "id": "aXQPby1qjHdQ",
    "outputId": "e288800b-14a8-4052-82a3-359abbca9391"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Pets',\n",
       "  'change',\n",
       "  'our',\n",
       "  'lives',\n",
       "  '&',\n",
       "  'amp',\n",
       "  ';',\n",
       "  'become',\n",
       "  'a',\n",
       "  'part',\n",
       "  'of',\n",
       "  'our',\n",
       "  'families',\n",
       "  '‚ù§Ô∏è',\n",
       "  'That',\n",
       "  \"'s\",\n",
       "  'why',\n",
       "  'our',\n",
       "  'members',\n",
       "  'offer',\n",
       "  'many',\n",
       "  'solutions',\n",
       "  'to',\n",
       "  'help',\n",
       "  'you',\n",
       "  'to',\n",
       "  'enjoy',\n",
       "  'a',\n",
       "  'long-lasting',\n",
       "  'bond',\n",
       "  'with',\n",
       "  'your',\n",
       "  'happy',\n",
       "  '&',\n",
       "  'amp',\n",
       "  ';',\n",
       "  'healthy',\n",
       "  'pet',\n",
       "  'üê±üê∂',\n",
       "  '#',\n",
       "  'MorethanMedicine',\n",
       "  '#',\n",
       "  'PetCare',\n",
       "  '#',\n",
       "  'PetsareFamily',\n",
       "  'https',\n",
       "  ':',\n",
       "  '//t.co/fZNIXge9a3'],\n",
       " ['Another',\n",
       "  'spot',\n",
       "  'of',\n",
       "  'our',\n",
       "  '#',\n",
       "  'morethanmedicine',\n",
       "  'bus',\n",
       "  'in',\n",
       "  '#',\n",
       "  'bristol',\n",
       "  'this',\n",
       "  'week',\n",
       "  '!',\n",
       "  'If',\n",
       "  'you',\n",
       "  'need',\n",
       "  'support',\n",
       "  'with',\n",
       "  'your',\n",
       "  'cancer',\n",
       "  'diagnosis',\n",
       "  'call',\n",
       "  'us',\n",
       "  'on',\n",
       "  '0303',\n",
       "  '3000',\n",
       "  '118',\n",
       "  '.',\n",
       "  '#',\n",
       "  'livingwellwithcancer',\n",
       "  'https',\n",
       "  ':',\n",
       "  '//t.co/eZGLz0BkXB'],\n",
       " ['What',\n",
       "  'a',\n",
       "  'great',\n",
       "  'team',\n",
       "  '\\u2066',\n",
       "  '@',\n",
       "  'HealthSourceOH\\u2069',\n",
       "  '\\u2066',\n",
       "  '@',\n",
       "  'Local12\\u2069',\n",
       "  '#',\n",
       "  'morethanmedicine',\n",
       "  'https',\n",
       "  ':',\n",
       "  '//t.co/g2YzMDUpVA'],\n",
       " ['What',\n",
       "  'a',\n",
       "  'great',\n",
       "  'team',\n",
       "  '\\u2066',\n",
       "  '@',\n",
       "  'HealthSourceOH\\u2069',\n",
       "  '\\u2066',\n",
       "  '@',\n",
       "  'Local12\\u2069',\n",
       "  '#',\n",
       "  'morethanmedicine',\n",
       "  'https',\n",
       "  ':',\n",
       "  '//t.co/g2YzMDUpVA'],\n",
       " ['What',\n",
       "  'a',\n",
       "  'great',\n",
       "  'team',\n",
       "  '\\u2066',\n",
       "  '@',\n",
       "  'HealthSourceOH\\u2069',\n",
       "  '\\u2066',\n",
       "  '@',\n",
       "  'Local12\\u2069',\n",
       "  '#',\n",
       "  'morethanmedicine',\n",
       "  'https',\n",
       "  ':',\n",
       "  '//t.co/g2YzMDUpVA'],\n",
       " ['What',\n",
       "  'a',\n",
       "  'great',\n",
       "  'team',\n",
       "  '\\u2066',\n",
       "  '@',\n",
       "  'HealthSourceOH\\u2069',\n",
       "  '\\u2066',\n",
       "  '@',\n",
       "  'Local12\\u2069',\n",
       "  '#',\n",
       "  'morethanmedicine',\n",
       "  'https',\n",
       "  ':',\n",
       "  '//t.co/g2YzMDUpVA'],\n",
       " ['Will',\n",
       "  'you',\n",
       "  'be',\n",
       "  'at',\n",
       "  '#',\n",
       "  'FIX19',\n",
       "  '?',\n",
       "  'Want',\n",
       "  'a',\n",
       "  'preview',\n",
       "  'of',\n",
       "  '@',\n",
       "  'AG_EM33',\n",
       "  'story',\n",
       "  '?',\n",
       "  'Then',\n",
       "  'check',\n",
       "  'back',\n",
       "  'Monday',\n",
       "  'for',\n",
       "  '#',\n",
       "  'ChangeOfHeart',\n",
       "  'where',\n",
       "  'we',\n",
       "  'sat',\n",
       "  'down',\n",
       "  'with',\n",
       "  'Alin',\n",
       "  'to',\n",
       "  'discuss',\n",
       "  'her',\n",
       "  'new',\n",
       "  'life',\n",
       "  'since',\n",
       "  'her',\n",
       "  'heart',\n",
       "  '#',\n",
       "  'transplant',\n",
       "  '#',\n",
       "  'MoreThanMedicine',\n",
       "  'https',\n",
       "  ':',\n",
       "  '//t.co/Xl9zjr7kZ1'],\n",
       " ['Will',\n",
       "  'you',\n",
       "  'be',\n",
       "  'at',\n",
       "  '#',\n",
       "  'FIX19',\n",
       "  '?',\n",
       "  'Want',\n",
       "  'a',\n",
       "  'preview',\n",
       "  'of',\n",
       "  '@',\n",
       "  'AG_EM33',\n",
       "  'story',\n",
       "  '?',\n",
       "  'Then',\n",
       "  'check',\n",
       "  'back',\n",
       "  'Monday',\n",
       "  'for',\n",
       "  '#',\n",
       "  'ChangeOfHeart',\n",
       "  'where',\n",
       "  'we',\n",
       "  'sat',\n",
       "  'down',\n",
       "  'with',\n",
       "  'Alin',\n",
       "  'to',\n",
       "  'discuss',\n",
       "  'her',\n",
       "  'new',\n",
       "  'life',\n",
       "  'since',\n",
       "  'her',\n",
       "  'heart',\n",
       "  '#',\n",
       "  'transplant',\n",
       "  '#',\n",
       "  'MoreThanMedicine',\n",
       "  'https',\n",
       "  ':',\n",
       "  '//t.co/Xl9zjr7kZ1'],\n",
       " ['Will',\n",
       "  'you',\n",
       "  'be',\n",
       "  'at',\n",
       "  '#',\n",
       "  'FIX19',\n",
       "  '?',\n",
       "  'Want',\n",
       "  'a',\n",
       "  'preview',\n",
       "  'of',\n",
       "  '@',\n",
       "  'AG_EM33',\n",
       "  'story',\n",
       "  '?',\n",
       "  'Then',\n",
       "  'check',\n",
       "  'back',\n",
       "  'Monday',\n",
       "  'for',\n",
       "  '#',\n",
       "  'ChangeOfHeart',\n",
       "  'where',\n",
       "  'we',\n",
       "  'sat',\n",
       "  'down',\n",
       "  'with',\n",
       "  'Alin',\n",
       "  'to',\n",
       "  'discuss',\n",
       "  'her',\n",
       "  'new',\n",
       "  'life',\n",
       "  'since',\n",
       "  'her',\n",
       "  'heart',\n",
       "  '#',\n",
       "  'transplant',\n",
       "  '#',\n",
       "  'MoreThanMedicine',\n",
       "  'https',\n",
       "  ':',\n",
       "  '//t.co/Xl9zjr7kZ1'],\n",
       " ['Will',\n",
       "  'you',\n",
       "  'be',\n",
       "  'at',\n",
       "  '#',\n",
       "  'FIX19',\n",
       "  '?',\n",
       "  'Want',\n",
       "  'a',\n",
       "  'preview',\n",
       "  'of',\n",
       "  '@',\n",
       "  'AG_EM33',\n",
       "  'story',\n",
       "  '?',\n",
       "  'Then',\n",
       "  'check',\n",
       "  'back',\n",
       "  'Monday',\n",
       "  'for',\n",
       "  '#',\n",
       "  'ChangeOfHeart',\n",
       "  'where',\n",
       "  'we',\n",
       "  'sat',\n",
       "  'down',\n",
       "  'with',\n",
       "  'Alin',\n",
       "  'to',\n",
       "  'discuss',\n",
       "  'her',\n",
       "  'new',\n",
       "  'life',\n",
       "  'since',\n",
       "  'her',\n",
       "  'heart',\n",
       "  '#',\n",
       "  'transplant',\n",
       "  '#',\n",
       "  'MoreThanMedicine',\n",
       "  'https',\n",
       "  ':',\n",
       "  '//t.co/Xl9zjr7kZ1']]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_tokenized_tweet = [word_tokenize(tweet) for tweet in first_10_tweets]\n",
    "\n",
    "default_tokenized_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 63,
     "status": "ok",
     "timestamp": 1748665007762,
     "user": {
      "displayName": "Kishan Prakash",
      "userId": "00782312557775250381"
     },
     "user_tz": -330
    },
    "id": "bsD0SXmKjOYQ",
    "outputId": "83680ff3-23df-4d7c-ce82-e1d35e8dcbb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TweetTokenizer : \n",
      "\n",
      "Tweet 1  Token : ['Pets', 'change', 'our', 'lives', '&', 'become', 'a', 'part', 'of', 'our', 'families', '‚ù§', 'Ô∏è', \"That's\", 'why', 'our', 'members', 'offer', 'many', 'solutions', 'to', 'help', 'you', 'to', 'enjoy', 'a', 'long-lasting', 'bond', 'with', 'your', 'happy', '&', 'healthy', 'pet', 'üê±', 'üê∂', '#MorethanMedicine', '#PetCare', '#PetsareFamily', 'https://t.co/fZNIXge9a3']\n",
      "Tweet 2  Token : ['Another', 'spot', 'of', 'our', '#morethanmedicine', 'bus', 'in', '#bristol', 'this', 'week', '!', 'If', 'you', 'need', 'support', 'with', 'your', 'cancer', 'diagnosis', 'call', 'us', 'on', '0303 3000', '118', '.', '#livingwellwithcancer', 'https://t.co/eZGLz0BkXB']\n",
      "Tweet 3  Token : ['What', 'a', 'great', 'team', '\\u2066', '@HealthSourceOH', '\\u2069', '\\u2066', '@Local12', '\\u2069', '#morethanmedicine', 'https://t.co/g2YzMDUpVA']\n",
      "Tweet 4  Token : ['What', 'a', 'great', 'team', '\\u2066', '@HealthSourceOH', '\\u2069', '\\u2066', '@Local12', '\\u2069', '#morethanmedicine', 'https://t.co/g2YzMDUpVA']\n",
      "Tweet 5  Token : ['What', 'a', 'great', 'team', '\\u2066', '@HealthSourceOH', '\\u2069', '\\u2066', '@Local12', '\\u2069', '#morethanmedicine', 'https://t.co/g2YzMDUpVA']\n",
      "Tweet 6  Token : ['What', 'a', 'great', 'team', '\\u2066', '@HealthSourceOH', '\\u2069', '\\u2066', '@Local12', '\\u2069', '#morethanmedicine', 'https://t.co/g2YzMDUpVA']\n",
      "Tweet 7  Token : ['Will', 'you', 'be', 'at', '#FIX19', '?', 'Want', 'a', 'preview', 'of', '@AG_EM33', 'story', '?', 'Then', 'check', 'back', 'Monday', 'for', '#ChangeOfHeart', 'where', 'we', 'sat', 'down', 'with', 'Alin', 'to', 'discuss', 'her', 'new', 'life', 'since', 'her', 'heart', '#transplant', '#MoreThanMedicine', 'https://t.co/Xl9zjr7kZ1']\n",
      "Tweet 8  Token : ['Will', 'you', 'be', 'at', '#FIX19', '?', 'Want', 'a', 'preview', 'of', '@AG_EM33', 'story', '?', 'Then', 'check', 'back', 'Monday', 'for', '#ChangeOfHeart', 'where', 'we', 'sat', 'down', 'with', 'Alin', 'to', 'discuss', 'her', 'new', 'life', 'since', 'her', 'heart', '#transplant', '#MoreThanMedicine', 'https://t.co/Xl9zjr7kZ1']\n",
      "Tweet 9  Token : ['Will', 'you', 'be', 'at', '#FIX19', '?', 'Want', 'a', 'preview', 'of', '@AG_EM33', 'story', '?', 'Then', 'check', 'back', 'Monday', 'for', '#ChangeOfHeart', 'where', 'we', 'sat', 'down', 'with', 'Alin', 'to', 'discuss', 'her', 'new', 'life', 'since', 'her', 'heart', '#transplant', '#MoreThanMedicine', 'https://t.co/Xl9zjr7kZ1']\n",
      "Tweet 10  Token : ['Will', 'you', 'be', 'at', '#FIX19', '?', 'Want', 'a', 'preview', 'of', '@AG_EM33', 'story', '?', 'Then', 'check', 'back', 'Monday', 'for', '#ChangeOfHeart', 'where', 'we', 'sat', 'down', 'with', 'Alin', 'to', 'discuss', 'her', 'new', 'life', 'since', 'her', 'heart', '#transplant', '#MoreThanMedicine', 'https://t.co/Xl9zjr7kZ1']\n"
     ]
    }
   ],
   "source": [
    "print(\"Using TweetTokenizer : \\n\")\n",
    "\n",
    "for i, token in enumerate(tweet_tokenized_tweet, start=1):\n",
    "\n",
    "    print(f\"Tweet {i}  Token : {token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1748665085552,
     "user": {
      "displayName": "Kishan Prakash",
      "userId": "00782312557775250381"
     },
     "user_tz": -330
    },
    "id": "VyA70iRBjnz9",
    "outputId": "eac4292e-0339-4d5e-cc00-f3a5b0f7b51d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Default Word Tokenizer (nltk.word_tokenize) : \n",
      "\n",
      "Tweet 1  Token : ['Pets', 'change', 'our', 'lives', '&', 'amp', ';', 'become', 'a', 'part', 'of', 'our', 'families', '‚ù§Ô∏è', 'That', \"'s\", 'why', 'our', 'members', 'offer', 'many', 'solutions', 'to', 'help', 'you', 'to', 'enjoy', 'a', 'long-lasting', 'bond', 'with', 'your', 'happy', '&', 'amp', ';', 'healthy', 'pet', 'üê±üê∂', '#', 'MorethanMedicine', '#', 'PetCare', '#', 'PetsareFamily', 'https', ':', '//t.co/fZNIXge9a3']\n",
      "Tweet 2  Token : ['Another', 'spot', 'of', 'our', '#', 'morethanmedicine', 'bus', 'in', '#', 'bristol', 'this', 'week', '!', 'If', 'you', 'need', 'support', 'with', 'your', 'cancer', 'diagnosis', 'call', 'us', 'on', '0303', '3000', '118', '.', '#', 'livingwellwithcancer', 'https', ':', '//t.co/eZGLz0BkXB']\n",
      "Tweet 3  Token : ['What', 'a', 'great', 'team', '\\u2066', '@', 'HealthSourceOH\\u2069', '\\u2066', '@', 'Local12\\u2069', '#', 'morethanmedicine', 'https', ':', '//t.co/g2YzMDUpVA']\n",
      "Tweet 4  Token : ['What', 'a', 'great', 'team', '\\u2066', '@', 'HealthSourceOH\\u2069', '\\u2066', '@', 'Local12\\u2069', '#', 'morethanmedicine', 'https', ':', '//t.co/g2YzMDUpVA']\n",
      "Tweet 5  Token : ['What', 'a', 'great', 'team', '\\u2066', '@', 'HealthSourceOH\\u2069', '\\u2066', '@', 'Local12\\u2069', '#', 'morethanmedicine', 'https', ':', '//t.co/g2YzMDUpVA']\n",
      "Tweet 6  Token : ['What', 'a', 'great', 'team', '\\u2066', '@', 'HealthSourceOH\\u2069', '\\u2066', '@', 'Local12\\u2069', '#', 'morethanmedicine', 'https', ':', '//t.co/g2YzMDUpVA']\n",
      "Tweet 7  Token : ['Will', 'you', 'be', 'at', '#', 'FIX19', '?', 'Want', 'a', 'preview', 'of', '@', 'AG_EM33', 'story', '?', 'Then', 'check', 'back', 'Monday', 'for', '#', 'ChangeOfHeart', 'where', 'we', 'sat', 'down', 'with', 'Alin', 'to', 'discuss', 'her', 'new', 'life', 'since', 'her', 'heart', '#', 'transplant', '#', 'MoreThanMedicine', 'https', ':', '//t.co/Xl9zjr7kZ1']\n",
      "Tweet 8  Token : ['Will', 'you', 'be', 'at', '#', 'FIX19', '?', 'Want', 'a', 'preview', 'of', '@', 'AG_EM33', 'story', '?', 'Then', 'check', 'back', 'Monday', 'for', '#', 'ChangeOfHeart', 'where', 'we', 'sat', 'down', 'with', 'Alin', 'to', 'discuss', 'her', 'new', 'life', 'since', 'her', 'heart', '#', 'transplant', '#', 'MoreThanMedicine', 'https', ':', '//t.co/Xl9zjr7kZ1']\n",
      "Tweet 9  Token : ['Will', 'you', 'be', 'at', '#', 'FIX19', '?', 'Want', 'a', 'preview', 'of', '@', 'AG_EM33', 'story', '?', 'Then', 'check', 'back', 'Monday', 'for', '#', 'ChangeOfHeart', 'where', 'we', 'sat', 'down', 'with', 'Alin', 'to', 'discuss', 'her', 'new', 'life', 'since', 'her', 'heart', '#', 'transplant', '#', 'MoreThanMedicine', 'https', ':', '//t.co/Xl9zjr7kZ1']\n",
      "Tweet 10  Token : ['Will', 'you', 'be', 'at', '#', 'FIX19', '?', 'Want', 'a', 'preview', 'of', '@', 'AG_EM33', 'story', '?', 'Then', 'check', 'back', 'Monday', 'for', '#', 'ChangeOfHeart', 'where', 'we', 'sat', 'down', 'with', 'Alin', 'to', 'discuss', 'her', 'new', 'life', 'since', 'her', 'heart', '#', 'transplant', '#', 'MoreThanMedicine', 'https', ':', '//t.co/Xl9zjr7kZ1']\n"
     ]
    }
   ],
   "source": [
    "print(\"Using Default Word Tokenizer (nltk.word_tokenize) : \\n\")\n",
    "\n",
    "for i, token in enumerate(default_tokenized_tweet, start=1):\n",
    "\n",
    "    print(f\"Tweet {i}  Token : {token}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oLuC5HhDtAlJ"
   },
   "source": [
    "# Text Cleaning / Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1748667763199,
     "user": {
      "displayName": "Kishan Prakash",
      "userId": "00782312557775250381"
     },
     "user_tz": -330
    },
    "id": "KqpEFwf-j6zp",
    "outputId": "b37d393c-ce94-4919-e00b-b62336b5cd6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Punctuation character !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "print(\"Punctuation character\", string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 58,
     "status": "ok",
     "timestamp": 1748668133873,
     "user": {
      "displayName": "Kishan Prakash",
      "userId": "00782312557775250381"
     },
     "user_tz": -330
    },
    "id": "6lazKrndviae",
    "outputId": "273032d9-4284-4fa8-d6fd-06d5ceb47bbe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Punctuation list\n",
      "!\n",
      "\"\n",
      "#\n",
      "$\n",
      "%\n",
      "&\n",
      "'\n",
      "(\n",
      ")\n",
      "*\n",
      "+\n",
      ",\n",
      "-\n",
      ".\n",
      "/\n",
      ":\n",
      ";\n",
      "<\n",
      "=\n",
      ">\n",
      "?\n",
      "@\n",
      "[\n",
      "\\\n",
      "]\n",
      "^\n",
      "_\n",
      "`\n",
      "{\n",
      "|\n",
      "}\n",
      "~\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nPunctuation list\")\n",
    "\n",
    "for punct in string.punctuation:\n",
    "\n",
    "    print(punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 37,
     "status": "ok",
     "timestamp": 1748668844577,
     "user": {
      "displayName": "Kishan Prakash",
      "userId": "00782312557775250381"
     },
     "user_tz": -330
    },
    "id": "7tXutW3eyOZV"
   },
   "outputs": [],
   "source": [
    "eu_defn = \"\"\"\n",
    "The European Union (EU) is a supranational political and economic union of 27 member states that are located primarily in Europe.[9][10] The union has a total area of 4,233,255 km2 (1,634,469 sq mi) and an estimated population of over 449 million as of 2024. The EU is often described as a sui generis political entity combining characteristics of both a federation and a confederation.[11][12]\n",
    "\n",
    "Containing 5.5% of the world population in 2023,[13] EU member states generated a nominal gross domestic product (GDP) of around ‚Ç¨17.935 trillion in 2024, accounting for approximately one sixth of global economic output.[14] Its cornerstone, the Customs Union, paved the way to establishing an internal single market based on standardised legal framework and legislation that applies in all member states in those matters, and only those matters, where the states have agreed to act as one. EU policies aim to ensure the free movement of people, goods, services and capital within the internal market;[15] enact legislation in justice and home affairs; and maintain common policies on trade,[16] agriculture,[17] fisheries and regional development.[18] Passport controls have been abolished for travel within the Schengen Area.[19] The eurozone is a group composed of the 20 EU member states that have fully implemented the EU's economic and monetary union and use the euro currency. Through the Common Foreign and Security Policy, the union has developed a role in external relations and defence. It maintains permanent diplomatic missions throughout the world and represents itself at the United Nations, the World Trade Organization, the G7 and the G20. Due to its global influence, the European Union has been described by some scholars as an emerging superpower.[20][21][22][needs update]\n",
    "\n",
    "The EU was established, along with its citizenship, when the Maastricht Treaty came into force in 1993, and was incorporated as an international legal juridical person[clarification needed] upon entry into force of the Treaty of Lisbon in 2009.[23] Its beginnings can be traced to the Inner Six states (Belgium, France, Italy, Luxembourg, the Netherlands, and West Germany) at the start of modern European integration in 1948, and to the Western Union, the International Authority for the Ruhr, the European Coal and Steel Community, the European Economic Community and the European Atomic Energy Community, which were established by treaties. These increasingly amalgamated bodies grew, with their legal successor the EU, both in size through the accessions of a further 22 states from 1973 to 2013, and in power through acquisitions of policy areas.\n",
    "\n",
    "In 2012, the EU was awarded the Nobel Peace Prize. In 2020, the United Kingdom became the only member state to leave the EU; ten countries are aspiring or negotiating to join it.\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 32,
     "status": "ok",
     "timestamp": 1748668845906,
     "user": {
      "displayName": "Kishan Prakash",
      "userId": "00782312557775250381"
     },
     "user_tz": -330
    },
    "id": "1l5Ytu_buIiP",
    "outputId": "4e3941a4-8f44-4e04-ff38-efbcf123251d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text without Punctuation  : \n",
      "\n",
      " The European Union  EU  is a supranational political and economic union of 27 member states that are located primarily in Europe  9  10  The union has a total area of 4 233 255 km2  1 634 469 sq mi  and an estimated population of over 449 million as of 2024  The EU is often described as a sui generis political entity combining characteristics of both a federation and a confederation  11  12   Containing 5 5  of the world population in 2023  13  EU member states generated a nominal gross domestic product  GDP  of around ‚Ç¨17 935 trillion in 2024  accounting for approximately one sixth of global economic output  14  Its cornerstone  the Customs Union  paved the way to establishing an internal single market based on standardised legal framework and legislation that applies in all member states in those matters  and only those matters  where the states have agreed to act as one  EU policies aim to ensure the free movement of people  goods  services and capital within the internal market  15  enact legislation in justice and home affairs  and maintain common policies on trade  16  agriculture  17  fisheries and regional development  18  Passport controls have been abolished for travel within the Schengen Area  19  The eurozone is a group composed of the 20 EU member states that have fully implemented the EU s economic and monetary union and use the euro currency  Through the Common Foreign and Security Policy  the union has developed a role in external relations and defence  It maintains permanent diplomatic missions throughout the world and represents itself at the United Nations  the World Trade Organization  the G7 and the G20  Due to its global influence  the European Union has been described by some scholars as an emerging superpower  20  21  22  needs update   The EU was established  along with its citizenship  when the Maastricht Treaty came into force in 1993  and was incorporated as an international legal juridical person clarification needed  upon entry into force of the Treaty of Lisbon in 2009  23  Its beginnings can be traced to the Inner Six states  Belgium  France  Italy  Luxembourg  the Netherlands  and West Germany  at the start of modern European integration in 1948  and to the Western Union  the International Authority for the Ruhr  the European Coal and Steel Community  the European Economic Community and the European Atomic Energy Community  which were established by treaties  These increasingly amalgamated bodies grew  with their legal successor the EU  both in size through the accessions of a further 22 states from 1973 to 2013  and in power through acquisitions of policy areas   In 2012  the EU was awarded the Nobel Peace Prize  In 2020  the United Kingdom became the only member state to leave the EU  ten countries are aspiring or negotiating to join it \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kisha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "eu_defn = eu_defn.replace(\"\\n\", \" \")\n",
    "\n",
    "for punct in string.punctuation:\n",
    "\n",
    "    eu_defn = eu_defn.replace(punct, \" \")\n",
    "\n",
    "print(\"Text without Punctuation  : \\n\")\n",
    "print(eu_defn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 41,
     "status": "ok",
     "timestamp": 1748668847964,
     "user": {
      "displayName": "Kishan Prakash",
      "userId": "00782312557775250381"
     },
     "user_tz": -330
    },
    "id": "LP4KF9h3vIWr",
    "outputId": "7f5cfaa0-1e67-404d-f1fb-b2a8adad6f84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text in Lower Case : \n",
      "\n",
      " the european union  eu  is a supranational political and economic union of 27 member states that are located primarily in europe  9  10  the union has a total area of 4 233 255 km2  1 634 469 sq mi  and an estimated population of over 449 million as of 2024  the eu is often described as a sui generis political entity combining characteristics of both a federation and a confederation  11  12   containing 5 5  of the world population in 2023  13  eu member states generated a nominal gross domestic product  gdp  of around ‚Ç¨17 935 trillion in 2024  accounting for approximately one sixth of global economic output  14  its cornerstone  the customs union  paved the way to establishing an internal single market based on standardised legal framework and legislation that applies in all member states in those matters  and only those matters  where the states have agreed to act as one  eu policies aim to ensure the free movement of people  goods  services and capital within the internal market  15  enact legislation in justice and home affairs  and maintain common policies on trade  16  agriculture  17  fisheries and regional development  18  passport controls have been abolished for travel within the schengen area  19  the eurozone is a group composed of the 20 eu member states that have fully implemented the eu s economic and monetary union and use the euro currency  through the common foreign and security policy  the union has developed a role in external relations and defence  it maintains permanent diplomatic missions throughout the world and represents itself at the united nations  the world trade organization  the g7 and the g20  due to its global influence  the european union has been described by some scholars as an emerging superpower  20  21  22  needs update   the eu was established  along with its citizenship  when the maastricht treaty came into force in 1993  and was incorporated as an international legal juridical person clarification needed  upon entry into force of the treaty of lisbon in 2009  23  its beginnings can be traced to the inner six states  belgium  france  italy  luxembourg  the netherlands  and west germany  at the start of modern european integration in 1948  and to the western union  the international authority for the ruhr  the european coal and steel community  the european economic community and the european atomic energy community  which were established by treaties  these increasingly amalgamated bodies grew  with their legal successor the eu  both in size through the accessions of a further 22 states from 1973 to 2013  and in power through acquisitions of policy areas   in 2012  the eu was awarded the nobel peace prize  in 2020  the united kingdom became the only member state to leave the eu  ten countries are aspiring or negotiating to join it \n"
     ]
    }
   ],
   "source": [
    "eu_defn = eu_defn.lower()\n",
    "\n",
    "print(\"Text in Lower Case : \\n\")\n",
    "print(eu_defn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 37,
     "status": "ok",
     "timestamp": 1748668850104,
     "user": {
      "displayName": "Kishan Prakash",
      "userId": "00782312557775250381"
     },
     "user_tz": -330
    },
    "id": "Xw59TGZuwhbQ",
    "outputId": "b101ef1b-0354-4041-dbcd-04312877ee3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Text : \n",
      "\n",
      "['the', 'european', 'union', 'eu', 'is', 'a', 'supranational', 'political', 'and', 'economic', 'union', 'of', '27', 'member', 'states', 'that', 'are', 'located', 'primarily', 'in', 'europe', '9', '10', 'the', 'union', 'has', 'a', 'total', 'area', 'of', '4', '233', '255', 'km2', '1', '634', '469', 'sq', 'mi', 'and', 'an', 'estimated', 'population', 'of', 'over', '449', 'million', 'as', 'of', '2024', 'the', 'eu', 'is', 'often', 'described', 'as', 'a', 'sui', 'generis', 'political', 'entity', 'combining', 'characteristics', 'of', 'both', 'a', 'federation', 'and', 'a', 'confederation', '11', '12', 'containing', '5', '5', 'of', 'the', 'world', 'population', 'in', '2023', '13', 'eu', 'member', 'states', 'generated', 'a', 'nominal', 'gross', 'domestic', 'product', 'gdp', 'of', 'around', '‚Ç¨17', '935', 'trillion', 'in', '2024', 'accounting', 'for', 'approximately', 'one', 'sixth', 'of', 'global', 'economic', 'output', '14', 'its', 'cornerstone', 'the', 'customs', 'union', 'paved', 'the', 'way', 'to', 'establishing', 'an', 'internal', 'single', 'market', 'based', 'on', 'standardised', 'legal', 'framework', 'and', 'legislation', 'that', 'applies', 'in', 'all', 'member', 'states', 'in', 'those', 'matters', 'and', 'only', 'those', 'matters', 'where', 'the', 'states', 'have', 'agreed', 'to', 'act', 'as', 'one', 'eu', 'policies', 'aim', 'to', 'ensure', 'the', 'free', 'movement', 'of', 'people', 'goods', 'services', 'and', 'capital', 'within', 'the', 'internal', 'market', '15', 'enact', 'legislation', 'in', 'justice', 'and', 'home', 'affairs', 'and', 'maintain', 'common', 'policies', 'on', 'trade', '16', 'agriculture', '17', 'fisheries', 'and', 'regional', 'development', '18', 'passport', 'controls', 'have', 'been', 'abolished', 'for', 'travel', 'within', 'the', 'schengen', 'area', '19', 'the', 'eurozone', 'is', 'a', 'group', 'composed', 'of', 'the', '20', 'eu', 'member', 'states', 'that', 'have', 'fully', 'implemented', 'the', 'eu', 's', 'economic', 'and', 'monetary', 'union', 'and', 'use', 'the', 'euro', 'currency', 'through', 'the', 'common', 'foreign', 'and', 'security', 'policy', 'the', 'union', 'has', 'developed', 'a', 'role', 'in', 'external', 'relations', 'and', 'defence', 'it', 'maintains', 'permanent', 'diplomatic', 'missions', 'throughout', 'the', 'world', 'and', 'represents', 'itself', 'at', 'the', 'united', 'nations', 'the', 'world', 'trade', 'organization', 'the', 'g7', 'and', 'the', 'g20', 'due', 'to', 'its', 'global', 'influence', 'the', 'european', 'union', 'has', 'been', 'described', 'by', 'some', 'scholars', 'as', 'an', 'emerging', 'superpower', '20', '21', '22', 'needs', 'update', 'the', 'eu', 'was', 'established', 'along', 'with', 'its', 'citizenship', 'when', 'the', 'maastricht', 'treaty', 'came', 'into', 'force', 'in', '1993', 'and', 'was', 'incorporated', 'as', 'an', 'international', 'legal', 'juridical', 'person', 'clarification', 'needed', 'upon', 'entry', 'into', 'force', 'of', 'the', 'treaty', 'of', 'lisbon', 'in', '2009', '23', 'its', 'beginnings', 'can', 'be', 'traced', 'to', 'the', 'inner', 'six', 'states', 'belgium', 'france', 'italy', 'luxembourg', 'the', 'netherlands', 'and', 'west', 'germany', 'at', 'the', 'start', 'of', 'modern', 'european', 'integration', 'in', '1948', 'and', 'to', 'the', 'western', 'union', 'the', 'international', 'authority', 'for', 'the', 'ruhr', 'the', 'european', 'coal', 'and', 'steel', 'community', 'the', 'european', 'economic', 'community', 'and', 'the', 'european', 'atomic', 'energy', 'community', 'which', 'were', 'established', 'by', 'treaties', 'these', 'increasingly', 'amalgamated', 'bodies', 'grew', 'with', 'their', 'legal', 'successor', 'the', 'eu', 'both', 'in', 'size', 'through', 'the', 'accessions', 'of', 'a', 'further', '22', 'states', 'from', '1973', 'to', '2013', 'and', 'in', 'power', 'through', 'acquisitions', 'of', 'policy', 'areas', 'in', '2012', 'the', 'eu', 'was', 'awarded', 'the', 'nobel', 'peace', 'prize', 'in', '2020', 'the', 'united', 'kingdom', 'became', 'the', 'only', 'member', 'state', 'to', 'leave', 'the', 'eu', 'ten', 'countries', 'are', 'aspiring', 'or', 'negotiating', 'to', 'join', 'it']\n"
     ]
    }
   ],
   "source": [
    "tokenized_eu_defn = word_tokenize(eu_defn)\n",
    "\n",
    "print(\"Tokenized Text : \\n\")\n",
    "print(tokenized_eu_defn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 32,
     "status": "ok",
     "timestamp": 1748668851789,
     "user": {
      "displayName": "Kishan Prakash",
      "userId": "00782312557775250381"
     },
     "user_tz": -330
    },
    "id": "n-vc98GwwpHR",
    "outputId": "c60ec52d-3a38-421b-8a62-eae896b483e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First 10 tokens : \n",
      "\n",
      "['the', 'european', 'union', 'eu', 'is', 'a', 'supranational', 'political', 'and', 'economic']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nFirst 10 tokens : \\n\")\n",
    "\n",
    "print(tokenized_eu_defn[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1748668936361,
     "user": {
      "displayName": "Kishan Prakash",
      "userId": "00782312557775250381"
     },
     "user_tz": -330
    },
    "id": "6OJXy3JSxyB9",
    "outputId": "0988fc66-f779-4262-cd2f-343566c29165"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 41),\n",
      " ('and', 21),\n",
      " ('of', 15),\n",
      " ('in', 14),\n",
      " ('eu', 10),\n",
      " ('a', 9),\n",
      " ('to', 9),\n",
      " ('union', 8),\n",
      " ('states', 7),\n",
      " ('european', 6),\n",
      " ('member', 5),\n",
      " ('as', 5),\n",
      " ('economic', 4),\n",
      " ('an', 4),\n",
      " ('its', 4),\n",
      " ('is', 3),\n",
      " ('that', 3),\n",
      " ('has', 3),\n",
      " ('world', 3),\n",
      " ('for', 3)]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "fdist = FreqDist(tokenized_eu_defn)\n",
    "\n",
    "top_20_tokens = fdist.most_common(20)\n",
    "\n",
    "pprint(top_20_tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hoNgYZDZym8s"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tVh5YaH3zchE"
   },
   "source": [
    "# Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 57,
     "status": "ok",
     "timestamp": 1748669558089,
     "user": {
      "displayName": "Kishan Prakash",
      "userId": "00782312557775250381"
     },
     "user_tz": -330
    },
    "id": "ASbYYJJ_zfYD",
    "outputId": "2fe206d6-1eb0-4884-c9fe-865ded8c17b8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kisha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLTK\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1748669589182,
     "user": {
      "displayName": "Kishan Prakash",
      "userId": "00782312557775250381"
     },
     "user_tz": -330
    },
    "id": "7kPXU-Qc0-u-"
   },
   "outputs": [],
   "source": [
    "paragraph =\"\"\"\n",
    "Language is one of the most powerful tools available to humanity. It allows us to communicate our thoughts, express emotions, and share ideas across time and space. From ancient manuscripts to modern AI-driven conversation systems, the evolution of language has been intertwined with human progress. Words carry meaning, but not all words contribute equally to understanding. This is where Natural Language Processing (NLP) techniques, such as stopword removal, become essential in extracting useful information from text.\n",
    "In computational linguistics, stopwords refer to commonly used words‚Äîsuch as \"the,\" \"is,\" \"and,\" or \"of\"‚Äîwhich often add little semantic value in text analysis. Removing stopwords helps reduce noise and improves the efficiency of NLP models, enabling them to focus on meaningful content. Consider a search engine processing billions of documents: by filtering out stopwords, it can enhance relevance and speed. However, stopword removal isn't always beneficial; in some cases, the presence of these words provides necessary context, making their elimination counterproductive.\n",
    "Beyond stopword removal, NLP encompasses other crucial techniques like stemming and lemmatization, which help normalize words by reducing them to their base forms. Additionally, named entity recognition (NER) identifies key figures, places, or organizations within text, while sentiment analysis determines whether a given passage conveys positive, negative, or neutral emotions. With the advent of deep learning and transformer models, NLP applications have expanded beyond traditional text processing to advanced tasks such as speech recognition and machine translation.\n",
    "As AI continues to evolve, NLP is transforming industries, enhancing virtual assistants, automating customer support, and even detecting patterns in medical texts. The ability to process and understand human language at scale has opened up endless possibilities, bridging the gap between humans and machines. Whether in academia, business, or creative storytelling, NLP-driven insights are shaping the future of communication in exciting ways.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 96,
     "status": "ok",
     "timestamp": 1748669869066,
     "user": {
      "displayName": "Kishan Prakash",
      "userId": "00782312557775250381"
     },
     "user_tz": -330
    },
    "id": "uSeDKw_O2HR7",
    "outputId": "172a32c4-ea18-4099-e529-57f30c9405ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence : \n",
      "Language is one of the most powerful tools available to humanity.\n",
      "Filtered Words : ['Language', 'one', 'powerful', 'tools', 'available', 'humanity', '.']\n",
      "Stemmed Words : ['languag', 'one', 'power', 'tool', 'avail', 'human', '.']\n",
      "\n",
      "Original Sentence : It allows us to communicate our thoughts, express emotions, and share ideas across time and space.\n",
      "Filtered Words : ['allows', 'us', 'communicate', 'thoughts', ',', 'express', 'emotions', ',', 'share', 'ideas', 'across', 'time', 'space', '.']\n",
      "Stemmed Words : ['allow', 'us', 'commun', 'thought', ',', 'express', 'emot', ',', 'share', 'idea', 'across', 'time', 'space', '.']\n",
      "\n",
      "Original Sentence : From ancient manuscripts to modern AI-driven conversation systems, the evolution of language has been intertwined with human progress.\n",
      "Filtered Words : ['ancient', 'manuscripts', 'modern', 'AI-driven', 'conversation', 'systems', ',', 'evolution', 'language', 'intertwined', 'human', 'progress', '.']\n",
      "Stemmed Words : ['ancient', 'manuscript', 'modern', 'ai-driven', 'convers', 'system', ',', 'evolut', 'languag', 'intertwin', 'human', 'progress', '.']\n",
      "\n",
      "Original Sentence : Words carry meaning, but not all words contribute equally to understanding.\n",
      "Filtered Words : ['Words', 'carry', 'meaning', ',', 'words', 'contribute', 'equally', 'understanding', '.']\n",
      "Stemmed Words : ['word', 'carri', 'mean', ',', 'word', 'contribut', 'equal', 'understand', '.']\n",
      "\n",
      "Original Sentence : This is where Natural Language Processing (NLP) techniques, such as stopword removal, become essential in extracting useful information from text.\n",
      "Filtered Words : ['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'techniques', ',', 'stopword', 'removal', ',', 'become', 'essential', 'extracting', 'useful', 'information', 'text', '.']\n",
      "Stemmed Words : ['natur', 'languag', 'process', '(', 'nlp', ')', 'techniqu', ',', 'stopword', 'remov', ',', 'becom', 'essenti', 'extract', 'use', 'inform', 'text', '.']\n",
      "\n",
      "Original Sentence : In computational linguistics, stopwords refer to commonly used words‚Äîsuch as \"the,\" \"is,\" \"and,\" or \"of\"‚Äîwhich often add little semantic value in text analysis.\n",
      "Filtered Words : ['computational', 'linguistics', ',', 'stopwords', 'refer', 'commonly', 'used', 'words‚Äîsuch', '``', ',', \"''\", '``', ',', \"''\", '``', ',', \"''\", '``', \"''\", '‚Äîwhich', 'often', 'add', 'little', 'semantic', 'value', 'text', 'analysis', '.']\n",
      "Stemmed Words : ['comput', 'linguist', ',', 'stopword', 'refer', 'commonli', 'use', 'words‚Äîsuch', '``', ',', \"''\", '``', ',', \"''\", '``', ',', \"''\", '``', \"''\", '‚Äîwhich', 'often', 'add', 'littl', 'semant', 'valu', 'text', 'analysi', '.']\n",
      "\n",
      "Original Sentence : Removing stopwords helps reduce noise and improves the efficiency of NLP models, enabling them to focus on meaningful content.\n",
      "Filtered Words : ['Removing', 'stopwords', 'helps', 'reduce', 'noise', 'improves', 'efficiency', 'NLP', 'models', ',', 'enabling', 'focus', 'meaningful', 'content', '.']\n",
      "Stemmed Words : ['remov', 'stopword', 'help', 'reduc', 'nois', 'improv', 'effici', 'nlp', 'model', ',', 'enabl', 'focu', 'meaning', 'content', '.']\n",
      "\n",
      "Original Sentence : Consider a search engine processing billions of documents: by filtering out stopwords, it can enhance relevance and speed.\n",
      "Filtered Words : ['Consider', 'search', 'engine', 'processing', 'billions', 'documents', ':', 'filtering', 'stopwords', ',', 'enhance', 'relevance', 'speed', '.']\n",
      "Stemmed Words : ['consid', 'search', 'engin', 'process', 'billion', 'document', ':', 'filter', 'stopword', ',', 'enhanc', 'relev', 'speed', '.']\n",
      "\n",
      "Original Sentence : However, stopword removal isn't always beneficial; in some cases, the presence of these words provides necessary context, making their elimination counterproductive.\n",
      "Filtered Words : ['However', ',', 'stopword', 'removal', \"n't\", 'always', 'beneficial', ';', 'cases', ',', 'presence', 'words', 'provides', 'necessary', 'context', ',', 'making', 'elimination', 'counterproductive', '.']\n",
      "Stemmed Words : ['howev', ',', 'stopword', 'remov', \"n't\", 'alway', 'benefici', ';', 'case', ',', 'presenc', 'word', 'provid', 'necessari', 'context', ',', 'make', 'elimin', 'counterproduct', '.']\n",
      "\n",
      "Original Sentence : Beyond stopword removal, NLP encompasses other crucial techniques like stemming and lemmatization, which help normalize words by reducing them to their base forms.\n",
      "Filtered Words : ['Beyond', 'stopword', 'removal', ',', 'NLP', 'encompasses', 'crucial', 'techniques', 'like', 'stemming', 'lemmatization', ',', 'help', 'normalize', 'words', 'reducing', 'base', 'forms', '.']\n",
      "Stemmed Words : ['beyond', 'stopword', 'remov', ',', 'nlp', 'encompass', 'crucial', 'techniqu', 'like', 'stem', 'lemmat', ',', 'help', 'normal', 'word', 'reduc', 'base', 'form', '.']\n",
      "\n",
      "Original Sentence : Additionally, named entity recognition (NER) identifies key figures, places, or organizations within text, while sentiment analysis determines whether a given passage conveys positive, negative, or neutral emotions.\n",
      "Filtered Words : ['Additionally', ',', 'named', 'entity', 'recognition', '(', 'NER', ')', 'identifies', 'key', 'figures', ',', 'places', ',', 'organizations', 'within', 'text', ',', 'sentiment', 'analysis', 'determines', 'whether', 'given', 'passage', 'conveys', 'positive', ',', 'negative', ',', 'neutral', 'emotions', '.']\n",
      "Stemmed Words : ['addit', ',', 'name', 'entiti', 'recognit', '(', 'ner', ')', 'identifi', 'key', 'figur', ',', 'place', ',', 'organ', 'within', 'text', ',', 'sentiment', 'analysi', 'determin', 'whether', 'given', 'passag', 'convey', 'posit', ',', 'neg', ',', 'neutral', 'emot', '.']\n",
      "\n",
      "Original Sentence : With the advent of deep learning and transformer models, NLP applications have expanded beyond traditional text processing to advanced tasks such as speech recognition and machine translation.\n",
      "Filtered Words : ['advent', 'deep', 'learning', 'transformer', 'models', ',', 'NLP', 'applications', 'expanded', 'beyond', 'traditional', 'text', 'processing', 'advanced', 'tasks', 'speech', 'recognition', 'machine', 'translation', '.']\n",
      "Stemmed Words : ['advent', 'deep', 'learn', 'transform', 'model', ',', 'nlp', 'applic', 'expand', 'beyond', 'tradit', 'text', 'process', 'advanc', 'task', 'speech', 'recognit', 'machin', 'translat', '.']\n",
      "\n",
      "Original Sentence : As AI continues to evolve, NLP is transforming industries, enhancing virtual assistants, automating customer support, and even detecting patterns in medical texts.\n",
      "Filtered Words : ['AI', 'continues', 'evolve', ',', 'NLP', 'transforming', 'industries', ',', 'enhancing', 'virtual', 'assistants', ',', 'automating', 'customer', 'support', ',', 'even', 'detecting', 'patterns', 'medical', 'texts', '.']\n",
      "Stemmed Words : ['ai', 'continu', 'evolv', ',', 'nlp', 'transform', 'industri', ',', 'enhanc', 'virtual', 'assist', ',', 'autom', 'custom', 'support', ',', 'even', 'detect', 'pattern', 'medic', 'text', '.']\n",
      "\n",
      "Original Sentence : The ability to process and understand human language at scale has opened up endless possibilities, bridging the gap between humans and machines.\n",
      "Filtered Words : ['ability', 'process', 'understand', 'human', 'language', 'scale', 'opened', 'endless', 'possibilities', ',', 'bridging', 'gap', 'humans', 'machines', '.']\n",
      "Stemmed Words : ['abil', 'process', 'understand', 'human', 'languag', 'scale', 'open', 'endless', 'possibl', ',', 'bridg', 'gap', 'human', 'machin', '.']\n",
      "\n",
      "Original Sentence : Whether in academia, business, or creative storytelling, NLP-driven insights are shaping the future of communication in exciting ways.\n",
      "Filtered Words : ['Whether', 'academia', ',', 'business', ',', 'creative', 'storytelling', ',', 'NLP-driven', 'insights', 'shaping', 'future', 'communication', 'exciting', 'ways', '.']\n",
      "Stemmed Words : ['whether', 'academia', ',', 'busi', ',', 'creativ', 'storytel', ',', 'nlp-driven', 'insight', 'shape', 'futur', 'commun', 'excit', 'way', '.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "\n",
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "\n",
    "    stemmed_words = [stemmer.stem(word) for word in filtered_words]\n",
    "\n",
    "    print(f\"Original Sentence : {sentences[i]}\")\n",
    "    print(f\"Filtered Words : {filtered_words}\")\n",
    "    print(f\"Stemmed Words : {stemmed_words}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1748670188846,
     "user": {
      "displayName": "Kishan Prakash",
      "userId": "00782312557775250381"
     },
     "user_tz": -330
    },
    "id": "oEGQ_j6P2U77",
    "outputId": "d1ecdcce-f23e-47a0-fe83-25995281de9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence : \n",
      "Language is one of the most powerful tools available to humanity.\n",
      "Filtered Words : ['Language', 'one', 'powerful', 'tools', 'available', 'humanity', '.']\n",
      "Lemmatized Words : ['Language', 'one', 'powerful', 'tool', 'available', 'humanity', '.']\n",
      "\n",
      "Original Sentence : It allows us to communicate our thoughts, express emotions, and share ideas across time and space.\n",
      "Filtered Words : ['allows', 'us', 'communicate', 'thoughts', ',', 'express', 'emotions', ',', 'share', 'ideas', 'across', 'time', 'space', '.']\n",
      "Lemmatized Words : ['allow', 'us', 'communicate', 'thoughts', ',', 'express', 'emotions', ',', 'share', 'ideas', 'across', 'time', 'space', '.']\n",
      "\n",
      "Original Sentence : From ancient manuscripts to modern AI-driven conversation systems, the evolution of language has been intertwined with human progress.\n",
      "Filtered Words : ['ancient', 'manuscripts', 'modern', 'AI-driven', 'conversation', 'systems', ',', 'evolution', 'language', 'intertwined', 'human', 'progress', '.']\n",
      "Lemmatized Words : ['ancient', 'manuscripts', 'modern', 'AI-driven', 'conversation', 'systems', ',', 'evolution', 'language', 'intertwine', 'human', 'progress', '.']\n",
      "\n",
      "Original Sentence : Words carry meaning, but not all words contribute equally to understanding.\n",
      "Filtered Words : ['Words', 'carry', 'meaning', ',', 'words', 'contribute', 'equally', 'understanding', '.']\n",
      "Lemmatized Words : ['Words', 'carry', 'mean', ',', 'word', 'contribute', 'equally', 'understand', '.']\n",
      "\n",
      "Original Sentence : This is where Natural Language Processing (NLP) techniques, such as stopword removal, become essential in extracting useful information from text.\n",
      "Filtered Words : ['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'techniques', ',', 'stopword', 'removal', ',', 'become', 'essential', 'extracting', 'useful', 'information', 'text', '.']\n",
      "Lemmatized Words : ['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'techniques', ',', 'stopword', 'removal', ',', 'become', 'essential', 'extract', 'useful', 'information', 'text', '.']\n",
      "\n",
      "Original Sentence : In computational linguistics, stopwords refer to commonly used words‚Äîsuch as \"the,\" \"is,\" \"and,\" or \"of\"‚Äîwhich often add little semantic value in text analysis.\n",
      "Filtered Words : ['computational', 'linguistics', ',', 'stopwords', 'refer', 'commonly', 'used', 'words‚Äîsuch', '``', ',', \"''\", '``', ',', \"''\", '``', ',', \"''\", '``', \"''\", '‚Äîwhich', 'often', 'add', 'little', 'semantic', 'value', 'text', 'analysis', '.']\n",
      "Lemmatized Words : ['computational', 'linguistics', ',', 'stopwords', 'refer', 'commonly', 'use', 'words‚Äîsuch', '``', ',', \"''\", '``', ',', \"''\", '``', ',', \"''\", '``', \"''\", '‚Äîwhich', 'often', 'add', 'little', 'semantic', 'value', 'text', 'analysis', '.']\n",
      "\n",
      "Original Sentence : Removing stopwords helps reduce noise and improves the efficiency of NLP models, enabling them to focus on meaningful content.\n",
      "Filtered Words : ['Removing', 'stopwords', 'helps', 'reduce', 'noise', 'improves', 'efficiency', 'NLP', 'models', ',', 'enabling', 'focus', 'meaningful', 'content', '.']\n",
      "Lemmatized Words : ['Removing', 'stopwords', 'help', 'reduce', 'noise', 'improve', 'efficiency', 'NLP', 'model', ',', 'enable', 'focus', 'meaningful', 'content', '.']\n",
      "\n",
      "Original Sentence : Consider a search engine processing billions of documents: by filtering out stopwords, it can enhance relevance and speed.\n",
      "Filtered Words : ['Consider', 'search', 'engine', 'processing', 'billions', 'documents', ':', 'filtering', 'stopwords', ',', 'enhance', 'relevance', 'speed', '.']\n",
      "Lemmatized Words : ['Consider', 'search', 'engine', 'process', 'billions', 'document', ':', 'filter', 'stopwords', ',', 'enhance', 'relevance', 'speed', '.']\n",
      "\n",
      "Original Sentence : However, stopword removal isn't always beneficial; in some cases, the presence of these words provides necessary context, making their elimination counterproductive.\n",
      "Filtered Words : ['However', ',', 'stopword', 'removal', \"n't\", 'always', 'beneficial', ';', 'cases', ',', 'presence', 'words', 'provides', 'necessary', 'context', ',', 'making', 'elimination', 'counterproductive', '.']\n",
      "Lemmatized Words : ['However', ',', 'stopword', 'removal', \"n't\", 'always', 'beneficial', ';', 'case', ',', 'presence', 'word', 'provide', 'necessary', 'context', ',', 'make', 'elimination', 'counterproductive', '.']\n",
      "\n",
      "Original Sentence : Beyond stopword removal, NLP encompasses other crucial techniques like stemming and lemmatization, which help normalize words by reducing them to their base forms.\n",
      "Filtered Words : ['Beyond', 'stopword', 'removal', ',', 'NLP', 'encompasses', 'crucial', 'techniques', 'like', 'stemming', 'lemmatization', ',', 'help', 'normalize', 'words', 'reducing', 'base', 'forms', '.']\n",
      "Lemmatized Words : ['Beyond', 'stopword', 'removal', ',', 'NLP', 'encompass', 'crucial', 'techniques', 'like', 'stem', 'lemmatization', ',', 'help', 'normalize', 'word', 'reduce', 'base', 'form', '.']\n",
      "\n",
      "Original Sentence : Additionally, named entity recognition (NER) identifies key figures, places, or organizations within text, while sentiment analysis determines whether a given passage conveys positive, negative, or neutral emotions.\n",
      "Filtered Words : ['Additionally', ',', 'named', 'entity', 'recognition', '(', 'NER', ')', 'identifies', 'key', 'figures', ',', 'places', ',', 'organizations', 'within', 'text', ',', 'sentiment', 'analysis', 'determines', 'whether', 'given', 'passage', 'conveys', 'positive', ',', 'negative', ',', 'neutral', 'emotions', '.']\n",
      "Lemmatized Words : ['Additionally', ',', 'name', 'entity', 'recognition', '(', 'NER', ')', 'identify', 'key', 'figure', ',', 'place', ',', 'organizations', 'within', 'text', ',', 'sentiment', 'analysis', 'determine', 'whether', 'give', 'passage', 'convey', 'positive', ',', 'negative', ',', 'neutral', 'emotions', '.']\n",
      "\n",
      "Original Sentence : With the advent of deep learning and transformer models, NLP applications have expanded beyond traditional text processing to advanced tasks such as speech recognition and machine translation.\n",
      "Filtered Words : ['advent', 'deep', 'learning', 'transformer', 'models', ',', 'NLP', 'applications', 'expanded', 'beyond', 'traditional', 'text', 'processing', 'advanced', 'tasks', 'speech', 'recognition', 'machine', 'translation', '.']\n",
      "Lemmatized Words : ['advent', 'deep', 'learn', 'transformer', 'model', ',', 'NLP', 'applications', 'expand', 'beyond', 'traditional', 'text', 'process', 'advance', 'task', 'speech', 'recognition', 'machine', 'translation', '.']\n",
      "\n",
      "Original Sentence : As AI continues to evolve, NLP is transforming industries, enhancing virtual assistants, automating customer support, and even detecting patterns in medical texts.\n",
      "Filtered Words : ['AI', 'continues', 'evolve', ',', 'NLP', 'transforming', 'industries', ',', 'enhancing', 'virtual', 'assistants', ',', 'automating', 'customer', 'support', ',', 'even', 'detecting', 'patterns', 'medical', 'texts', '.']\n",
      "Lemmatized Words : ['AI', 'continue', 'evolve', ',', 'NLP', 'transform', 'industries', ',', 'enhance', 'virtual', 'assistants', ',', 'automate', 'customer', 'support', ',', 'even', 'detect', 'pattern', 'medical', 'texts', '.']\n",
      "\n",
      "Original Sentence : The ability to process and understand human language at scale has opened up endless possibilities, bridging the gap between humans and machines.\n",
      "Filtered Words : ['ability', 'process', 'understand', 'human', 'language', 'scale', 'opened', 'endless', 'possibilities', ',', 'bridging', 'gap', 'humans', 'machines', '.']\n",
      "Lemmatized Words : ['ability', 'process', 'understand', 'human', 'language', 'scale', 'open', 'endless', 'possibilities', ',', 'bridge', 'gap', 'humans', 'machine', '.']\n",
      "\n",
      "Original Sentence : Whether in academia, business, or creative storytelling, NLP-driven insights are shaping the future of communication in exciting ways.\n",
      "Filtered Words : ['Whether', 'academia', ',', 'business', ',', 'creative', 'storytelling', ',', 'NLP-driven', 'insights', 'shaping', 'future', 'communication', 'exciting', 'ways', '.']\n",
      "Lemmatized Words : ['Whether', 'academia', ',', 'business', ',', 'creative', 'storytelling', ',', 'NLP-driven', 'insights', 'shape', 'future', 'communication', 'excite', 'ways', '.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "processed_sentences = []\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word, pos='v') for word in filtered_words]\n",
    "\n",
    "    print(f\"Original Sentence : {sentences[i]}\")\n",
    "    print(f\"Filtered Words : {filtered_words}\")\n",
    "    print(f\"Lemmatized Words : {lemmatized_words}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1748670330346,
     "user": {
      "displayName": "Kishan Prakash",
      "userId": "00782312557775250381"
     },
     "user_tz": -330
    },
    "id": "Tfal9v5T3YuU",
    "outputId": "ec1dacfc-815c-4ed8-a64e-a0dfd36b2451"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Whether academia , business , creative storytelling , NLP-driven insights shape future communication excite ways .']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_sentence = ' '.join(lemmatized_words)\n",
    "\n",
    "processed_sentences.append(processed_sentence)\n",
    "\n",
    "processed_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 620,
     "status": "ok",
     "timestamp": 1748670725168,
     "user": {
      "displayName": "Kishan Prakash",
      "userId": "00782312557775250381"
     },
     "user_tz": -330
    },
    "id": "ZBZJlNTB34Gg",
    "outputId": "e7a2e031-256b-4865-8fe4-692bf66cc829"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word : \n",
      " \t Is Stopword : False\n",
      "Word : Language \t Is Stopword : False\n",
      "Word : is \t Is Stopword : True\n",
      "Word : one \t Is Stopword : True\n",
      "Word : of \t Is Stopword : True\n",
      "Word : the \t Is Stopword : True\n",
      "Word : most \t Is Stopword : True\n",
      "Word : powerful \t Is Stopword : False\n",
      "Word : tools \t Is Stopword : False\n",
      "Word : available \t Is Stopword : False\n",
      "Word : to \t Is Stopword : True\n",
      "Word : humanity \t Is Stopword : False\n",
      "Word : . \t Is Stopword : False\n",
      "Word : \n",
      " \t Is Stopword : False\n",
      "Word : It \t Is Stopword : True\n",
      "Word : allows \t Is Stopword : False\n",
      "Word : us \t Is Stopword : True\n",
      "Word : to \t Is Stopword : True\n",
      "Word : communicate \t Is Stopword : False\n",
      "Word : our \t Is Stopword : True\n",
      "Word : thoughts \t Is Stopword : False\n",
      "Word : , \t Is Stopword : False\n",
      "Word : express \t Is Stopword : False\n",
      "Word : emotions \t Is Stopword : False\n",
      "Word : , \t Is Stopword : False\n",
      "Word : and \t Is Stopword : True\n",
      "Word : share \t Is Stopword : False\n",
      "Word : ideas \t Is Stopword : False\n",
      "Word : across \t Is Stopword : True\n",
      "Word : time \t Is Stopword : False\n",
      "Word : and \t Is Stopword : True\n",
      "Word : space \t Is Stopword : False\n",
      "Word : . \t Is Stopword : False\n",
      "Word : \n",
      " \t Is Stopword : False\n",
      "Word : From \t Is Stopword : True\n",
      "Word : ancient \t Is Stopword : False\n",
      "Word : manuscripts \t Is Stopword : False\n",
      "Word : to \t Is Stopword : True\n",
      "Word : modern \t Is Stopword : False\n",
      "Word : AI \t Is Stopword : False\n",
      "Word : - \t Is Stopword : False\n",
      "Word : driven \t Is Stopword : False\n",
      "Word : conversation \t Is Stopword : False\n",
      "Word : systems \t Is Stopword : False\n",
      "Word : , \t Is Stopword : False\n",
      "Word : \n",
      " \t Is Stopword : False\n",
      "Word : the \t Is Stopword : True\n",
      "Word : evolution \t Is Stopword : False\n",
      "Word : of \t Is Stopword : True\n",
      "Word : language \t Is Stopword : False\n",
      "Word : has \t Is Stopword : True\n",
      "Word : been \t Is Stopword : True\n",
      "Word : intertwined \t Is Stopword : False\n",
      "Word : with \t Is Stopword : True\n",
      "Word : human \t Is Stopword : False\n",
      "Word : progress \t Is Stopword : False\n",
      "Word : . \t Is Stopword : False\n",
      "Word : \n",
      " \t Is Stopword : False\n",
      "Word : Words \t Is Stopword : False\n",
      "Word : carry \t Is Stopword : False\n",
      "Word : meaning \t Is Stopword : False\n",
      "Word : , \t Is Stopword : False\n",
      "Word : but \t Is Stopword : True\n",
      "Word : not \t Is Stopword : True\n",
      "Word : all \t Is Stopword : True\n",
      "Word : words \t Is Stopword : False\n",
      "Word : contribute \t Is Stopword : False\n",
      "Word : equally \t Is Stopword : False\n",
      "Word : to \t Is Stopword : True\n",
      "Word : understanding \t Is Stopword : False\n",
      "Word : . \t Is Stopword : False\n",
      "Word : \n",
      " \t Is Stopword : False\n",
      "Word : This \t Is Stopword : True\n",
      "Word : is \t Is Stopword : True\n",
      "Word : where \t Is Stopword : True\n",
      "Word : Natural \t Is Stopword : False\n",
      "Word : Language \t Is Stopword : False\n",
      "Word : Processing \t Is Stopword : False\n",
      "Word : ( \t Is Stopword : False\n",
      "Word : NLP \t Is Stopword : False\n",
      "Word : ) \t Is Stopword : False\n",
      "Word : techniques \t Is Stopword : False\n",
      "Word : , \t Is Stopword : False\n",
      "Word : \n",
      " \t Is Stopword : False\n",
      "Word : such \t Is Stopword : True\n",
      "Word : as \t Is Stopword : True\n",
      "Word : stopword \t Is Stopword : False\n",
      "Word : removal \t Is Stopword : False\n",
      "Word : , \t Is Stopword : False\n",
      "Word : become \t Is Stopword : True\n",
      "Word : essential \t Is Stopword : False\n",
      "Word : in \t Is Stopword : True\n",
      "Word : extracting \t Is Stopword : False\n",
      "Word : useful \t Is Stopword : False\n",
      "Word : information \t Is Stopword : False\n",
      "Word : from \t Is Stopword : True\n",
      "Word : text \t Is Stopword : False\n",
      "Word : . \t Is Stopword : False\n"
     ]
    }
   ],
   "source": [
    "# spaCy\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "sentence = \"\"\"\n",
    "Language is one of the most powerful tools available to humanity.\n",
    "It allows us to communicate our thoughts, express emotions, and share ideas across time and space.\n",
    "From ancient manuscripts to modern AI-driven conversation systems,\n",
    "the evolution of language has been intertwined with human progress.\n",
    "Words carry meaning, but not all words contribute equally to understanding.\n",
    "This is where Natural Language Processing (NLP) techniques,\n",
    "such as stopword removal, become essential in extracting useful information from text.\"\"\"\n",
    "\n",
    "doc = nlp(sentence)\n",
    "\n",
    "for token in doc:\n",
    "\n",
    "    print(f\"Word : {token.text} \\t Is Stopword : {token.is_stop}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 62,
     "status": "ok",
     "timestamp": 1748670806289,
     "user": {
      "displayName": "Kishan Prakash",
      "userId": "00782312557775250381"
     },
     "user_tz": -330
    },
    "id": "WVG-cC6T5bhN",
    "outputId": "b5336add-94db-454f-bc68-4779a3a97bd0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'was', 'i', 'his', 'quite', 'otherwise', 'eight', \"'ll\", 'almost', 'move', 'such', 'what', 'last', 'thereafter', 'it', 'you', 'themselves', 'indeed', 'behind', 'though', 'only', \"'re\", 'amount', 'five', 'although', 'towards', 'when', 'in', 'whither', 'per', 'perhaps', 'the', 'yet', 'an', 'whatever', 'anyway', 'ten', 'off', 'with', 'becomes', 'others', 'further', 'her', '‚Äôve', 'fifty', 'also', 'since', 'whereas', 'she', 'herein', 'top', 'through', 'anyone', 'whom', 'than', 'yourselves', 'anyhow', 'so', 'anything', 'out', 'four', 'under', 'all', 'else', 'sometimes', 'your', 'them', 'n‚Äôt', 'see', 'twenty', 'itself', 'into', 'many', 'even', 'bottom', 'several', 'this', 'together', 'get', 'whereupon', 'yourself', 'toward', 'just', 'take', 'some', 'while', 'unless', '‚Äôll', 'hence', 'among', \"'m\", 'another', 'their', 'formerly', 'anywhere', 'whose', 'say', 'put', 'of', 'how', 'herself', 'within', 'they', 'who', 'would', '‚Äòd', 'wherever', 'as', 'often', 'is', 'been', 'same', 'least', 'nevertheless', '‚Äòm', 'n‚Äòt', 'were', 'moreover', 'because', 'everywhere', 'that', 'whoever', 'whence', 'without', 'but', 'over', 'latter', 'upon', 'thereby', 'done', \"n't\", 'few', 'during', 'nor', 'before', 'nobody', '‚Äôs', 'always', 'enough', 'becoming', 'via', 'more', 'by', 'against', 'beside', 'own', 'here', 'its', 'amongst', 'somewhere', 'seems', 'became', 'doing', 'after', 'less', '‚Äôre', 'seemed', 'had', 'forty', 'again', 'two', 'hundred', 'us', 'full', 'go', 'ca', 'name', 'sixty', '‚Äòre', 'me', 'he', 're', 'a', 'each', '‚Äôm', 'my', 'thence', 'give', 'beforehand', 'thru', 'hereafter', 'much', 'any', 'ever', 'whole', 'rather', 'next', 'beyond', 'every', 'do', 'everything', 'have', 'does', 'those', 'well', 'himself', 'whenever', 'can', 'somehow', 'twelve', 'above', 'now', 'using', 'where', 'or', 'eleven', 'has', 'be', 'besides', 'meanwhile', 'are', 'alone', 'ours', 'these', 'yours', 'therein', 'then', 'serious', 'nothing', 'except', '‚Äòve', 'nowhere', 'thereupon', \"'s\", 'either', 'onto', 'please', 'empty', 'keep', 'and', 'being', '‚Äòll', 'seem', 'up', 'across', 'him', 'our', 'thus', 'am', 'make', 'to', 'wherein', 'myself', 'latterly', \"'ve\", 'there', 'at', 'throughout', 'front', 'if', 'various', 'mostly', 'used', 'whether', 'hereupon', 'show', 'between', 'should', 'someone', 'whereafter', 'below', 'three', 'none', 'part', 'why', 'afterwards', 'one', 'nine', 'other', 'about', 'become', 'did', 'down', 'will', \"'d\", 'made', 'former', 'for', 'along', 'most', 'could', 'from', 'however', 'must', 'not', 'fifteen', 'already', 'whereby', 'regarding', 'side', 'due', 'elsewhere', 'therefore', 'until', 'sometime', 'still', 'too', 'on', 'third', 'back', 'hereby', 'first', 'hers', 'might', 'once', 'really', 'may', 'namely', 'around', 'mine', '‚Äòs', 'ourselves', 'six', 'cannot', 'very', 'both', 'no', 'something', 'seeming', 'which', 'call', '‚Äôd', 'noone', 'everyone', 'we', 'never', 'neither'}\n",
      "\n",
      "326\n"
     ]
    }
   ],
   "source": [
    "# Default Stop words\n",
    "print(nlp.Defaults.stop_words)\n",
    "print()\n",
    "print(len(nlp.Defaults.stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1748670867329,
     "user": {
      "displayName": "Kishan Prakash",
      "userId": "00782312557775250381"
     },
     "user_tz": -330
    },
    "id": "ieSP8fd05vds",
    "outputId": "be878741-30a4-4a0a-e010-a14a5cc4e613"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# check Default Stop words\n",
    "print(nlp.vocab['is'].is_stop)\n",
    "print(nlp.vocab['student'].is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1748671068563,
     "user": {
      "displayName": "Kishan Prakash",
      "userId": "00782312557775250381"
     },
     "user_tz": -330
    },
    "id": "9eCFFrS95-Xs",
    "outputId": "be297d20-ec3e-491a-b51f-20437c2aaa55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "327\n"
     ]
    }
   ],
   "source": [
    "# Add\n",
    "\n",
    "nlp.Defaults.stop_words.add('btw')\n",
    "nlp.vocab['btw'].is_stop = True\n",
    "\n",
    "print(len(nlp.Defaults.stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1748671108304,
     "user": {
      "displayName": "Kishan Prakash",
      "userId": "00782312557775250381"
     },
     "user_tz": -330
    },
    "id": "wVbUMNqc6vgb",
    "outputId": "5327c0fa-8107-454c-a36a-dae0a04711bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326\n"
     ]
    }
   ],
   "source": [
    "# Remove\n",
    "\n",
    "nlp.Defaults.stop_words.remove('beyond')\n",
    "nlp.vocab['beyond'].is_stop = False\n",
    "\n",
    "print(len(nlp.Defaults.stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SCJY5PSX65Nd"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM2SkMxlMxeGKaJUtCj10UZ",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
